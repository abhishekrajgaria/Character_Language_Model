{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--output_dir OUTPUT_DIR]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --ip=127.0.0.1 --stdin=9018 --control=9016 --hb=9015 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"b446a990-2002-428d-a7a6-ff8b7dad97c4\" --shell=9017 --transport=\"tcp\" --iopub=9019 --f=/uufs/chpc.utah.edu/common/home/u1471428/.local/share/jupyter/runtime/kernel-v2-2327565UqsIJXW28Aip.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import model\n",
    "\n",
    "from custom_data_loader import *\n",
    "from main import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version: 2.0.1+cu117\n",
      "Device name: NVIDIA GeForce GT 1030\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Pytorch version: {torch.__version__}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_perf_dict = {\n",
    "    \"metric\": -1,\n",
    "    \"model_param\": None,\n",
    "    \"optim_param\": None,\n",
    "    \"epoch\": 0,\n",
    "    \"learning_rate\": 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "learning_rates = [0.01]  # , 0.001, 0.0001]\n",
    "\n",
    "seq_len = 500\n",
    "\n",
    "train_folder = \"./mp3_release/data/train\"\n",
    "dev_folder = \"./mp3_zrelease/data/dev\"\n",
    "test_folder = \"./mp3_release/data/test\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, rev_vocab = getVocab(\"./mp3_release/data/vocab.pkl\")\n",
    "ignore_index = vocab[\"[PAD]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_char_data = getCharData(train_folder, vocab)\n",
    "# dev_char_data = getCharData(dev_folder, vocab)\n",
    "test_char_data = getCharData(test_folder, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = getWeights(vocab, train_char_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "879963\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train_dataset = getData(vocab, train_char_data, 500)\n",
    "dev_dataset = getData(vocab, dev_char_data, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154145\n",
      "torch.Size([167803, 500])\n",
      "torch.Size([167803, 500])\n"
     ]
    }
   ],
   "source": [
    "test_dataset = getData(vocab, test_char_data,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = model.LMModel(len(vocab), num_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadModel(model_instance, model_path):\n",
    "    checkpoint = torch.load(model_path)\n",
    "    # print(checkpoint)\n",
    "    model_instance.load_state_dict(checkpoint[\"model_param\"])\n",
    "    print(\n",
    "        f\"\"\"Dev_LAS of loaded model: {checkpoint[\"dev_metric\"]} at epoch {checkpoint[\"epoch\"]} with learning rate {checkpoint[\"learning_rate\"]}\"\"\"\n",
    "    )\n",
    "    return model_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev_LAS of loaded model: 1.3527647256851196 at epoch 0 with learning rate 0.01\n"
     ]
    }
   ],
   "source": [
    "best_model_path = \"/scratch/general/vast/u1471428/cs6957/assignment3/models/layer_2/0.01_0\"\n",
    "best_model = loadModel(test_model, best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "718986\n"
     ]
    }
   ],
   "source": [
    "num_param = sum( p . numel () for p in test_model.parameters () )\n",
    "\n",
    "print(num_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateModel(model, dataset):\n",
    "    model = model.to(device)\n",
    "\n",
    "    dataLoader = torch.utils.data.DataLoader(\n",
    "            dataset, 128\n",
    "        )\n",
    "    with torch.no_grad():\n",
    "        loss_function = nn.CrossEntropyLoss(\n",
    "            weight=weights, ignore_index=ignore_index, reduce=False\n",
    "        )\n",
    "\n",
    "        loss_function = loss_function.to(device)\n",
    "        model.eval()\n",
    "        perplexity = 0\n",
    "\n",
    "        total_len = 0\n",
    "        for input, target in dataLoader:\n",
    "            batch_size = input.shape[0]\n",
    "            total_len += batch_size\n",
    "            seq_len = input.shape[1]\n",
    "\n",
    "            output = model(input.to(device))\n",
    "            # sen_len * batch_size * vocab_size\n",
    "            # print(\"output \", output.shape)\n",
    "            # target = target.permute(1, 0)\n",
    "            # print(\"target\", target.shape)\n",
    "\n",
    "            output = output.view(-1, output.shape[-1])\n",
    "            target = target.reshape(-1)\n",
    "\n",
    "            char_level_loss = loss_function(\n",
    "                output, target.to(device)\n",
    "            )  # seq_len x batch_size\n",
    "\n",
    "            # print(char_level_loss.shape)\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                seq_loss = 0\n",
    "                for j in range(i * seq_len, (i + 1) * seq_len):\n",
    "                    seq_loss += char_level_loss[j]\n",
    "\n",
    "                seq_loss = seq_loss / seq_len\n",
    "                perplexity += 2**seq_loss\n",
    "            del char_level_loss\n",
    "\n",
    "        perplexity = perplexity / total_len\n",
    "\n",
    "        print(f\"Perplexity on test dataset: {perplexity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uufs/chpc.utah.edu/common/home/u1471428/miniconda3/envs/asg/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity on test dataset: 1.3065770864486694\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "evaluateModel(test_model, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(\"*****************\")\n",
    "    print(\"learning_rate\", lr)\n",
    "    trainer = Trainer(\n",
    "        model1,\n",
    "        train_dataset,\n",
    "        dev_dataset,\n",
    "        weights,\n",
    "        ignore_index,\n",
    "        batch_size=128,\n",
    "        epochs=5,\n",
    "    )\n",
    "    trainer.trainModel(lr)\n",
    "    print(\"*****************\")\n",
    "    print()\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"--- %s seconds ---\" % (end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"The little boy was\",\n",
    "    \"Once upon a time in\",\n",
    "    \"With the target in\",\n",
    "    \"Capitals are big cities. For example,\",\n",
    "    \"A cheap alternative to\",\n",
    "    \"Salt Lake City in Utah is filled with Hobos\",\n",
    "    \"Johnny Johnny yes papa\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_utils import convert_sent2idx\n",
    "sent_data = convert_sent2idx(sentences,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 18])\n"
     ]
    }
   ],
   "source": [
    "tensor_sent_data = []\n",
    "for data in sent_data:\n",
    "    tensor_data = torch.tensor(data)\n",
    "    tensor_data = torch.unsqueeze(tensor_data,dim=0)\n",
    "    tensor_sent_data.append(tensor_data)\n",
    "\n",
    "print(tensor_sent_data[0].shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version: 2.0.1+cu117\n",
      "Device name: NVIDIA GeForce GT 1030\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Pytorch version: {torch.__version__}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU available.\")\n",
    "\n",
    "\n",
    "class LMModel2(nn.Module):\n",
    "    def __init__(self, vocab_dim, emb_dim=50, hidden_dim=200, num_layers=1):\n",
    "        super(LMModel2, self).__init__()\n",
    "        self.vocab_dim = vocab_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.projection_layer = nn.Embedding(vocab_dim, emb_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.emb_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 300, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300, self.vocab_dim, bias=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        # print(input.shape)\n",
    "        self.batch_size = input.shape[0]\n",
    "        for i in range(200):\n",
    "            input_emb = self.projection_layer(input)\n",
    "\n",
    "            \n",
    "\n",
    "            h_0 = Variable(\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).to(device)\n",
    "            )\n",
    "            c_0 = Variable(\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).to(device)\n",
    "            )\n",
    "\n",
    "            lstm_output, hidden = self.lstm(input_emb, (h_0, c_0))\n",
    "            # print(lstm_output.shape)\n",
    "\n",
    "            next_char_prob = self.fc_layer(lstm_output[:,-1,:])\n",
    "\n",
    "            # print(next_char_prob.shape)\n",
    "            # print(next_char_prob)\n",
    "\n",
    "            next_char_prob = nn.Softmax(dim=1)(next_char_prob)\n",
    "\n",
    "            # print(next_char_prob.shape)\n",
    "            # print(next_char_prob)\n",
    "\n",
    "\n",
    "\n",
    "            sample = torch.multinomial(next_char_prob,1)\n",
    "\n",
    "            # print(sample)\n",
    "            # print(sample.shape)\n",
    "\n",
    "            input = torch.cat((input,sample),dim=1)\n",
    "            # print(input.shape)\n",
    "\n",
    "\n",
    "\n",
    "        return input\n",
    "\n",
    "    # def forward_step(self, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev_LAS of loaded model: 1.2572855949401855 at epoch 0 with learning rate 0.01\n"
     ]
    }
   ],
   "source": [
    "# from model2 import LMModel2\n",
    "model_2 = LMModel2(len(vocab),num_layers=2)\n",
    "best_model_path = \"/scratch/general/vast/u1471428/cs6957/assignment3/models/layer_2/0.01_0\"\n",
    "best_model = loadModel(model_2, best_model_path)\n",
    "best_model = best_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The little boy was thank or disappanching, with the Siviente Liceivage, (the ladies in the fallance.=\n",
      " philosophy. The time dominated itself on angles found. The verulous elefficer submitter bar been proper Soul to see\n",
      "****************************************\n",
      "Once upon a time in his cigar collection,\n",
      " aparted with girl of musima’s datchard, with the doctor limitiestate of trusts a step down the open first that was a paperd - will establish thousted, to shight or catain and v\n",
      "****************************************\n",
      "With the target into the outer opposite lants followed the beam.\n",
      ".ory. The Englissy of Himself, N. Hussy and Athenshillonmobirt Mit gave the seats, and an I was some thousand array, of the misery—we better in Manow, me\n",
      "****************************************\n",
      "Capitals are big cities. For example, and there \"write.\n",
      " accepting it started leave to taggest indignation; I must be dimsy is for work in the fact you are that lack is insisted by reply out of the new Highest. The down across the immodo\n",
      "****************************************\n",
      "A cheap alternative to planting immusing-house and with the events firstly times. Be in the loss, Drave L. never, The Mill and espicefort--thot \"sexual sepurity of Mr. B. Thor mortal\" to light, hearsoon far linen out by th\n",
      "****************************************\n",
      "Salt Lake City in Utah is filled with Hobosson of Joan Elemshoffna, Siischet, UNS\n",
      ". 163. Sains O letter \"Lichess’d,_ The Count girl, and we may Safe; Thousance to that of the labours the argument Groacheast ce exceeded to have clearly to the t\n",
      "****************************************\n",
      "Johnny Johnny yes papa lan detemploai; hoili, ditő; on yet and a billials which brounds then a hinting than to do the narralifield starter over aner of mind a tobo along the latest slowl (Bushiux uur of trouble) isn't vita\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    best_model.eval()\n",
    "    for sent in tensor_sent_data: \n",
    "        extended_input = best_model(sent.to(device))\n",
    "        # print(extended_input.shape)\n",
    "        extended_input = extended_input.view(-1)\n",
    "        # print(extended_input.shape)\n",
    "        gen_seq = \"\"\n",
    "        for ind in extended_input:\n",
    "            gen_seq += rev_vocab[ind.item()]\n",
    "        print(gen_seq)\n",
    "        print(\"****************************************\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    }
   ],
   "source": [
    "print(rev_vocab[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
